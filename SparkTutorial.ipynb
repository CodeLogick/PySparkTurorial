{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d224ff69-a398-466b-8799-7cdae24c9179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/02 10:51:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "|Sanjay| 35|\n",
      "|  Ajay| 39|\n",
      "| Vijay| 60|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkTutorial').master('local[*]').getOrCreate()\n",
    "\n",
    "data = [\n",
    "    ('Sanjay', 35),\n",
    "    ('Ajay', 39),\n",
    "    ('Vijay', 60)\n",
    "]\n",
    "df = spark.createDataFrame(data).toDF('Name', 'Age')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfd0db8-9033-42b4-9922-21cb947ec868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          Timestamp|\n",
      "+-------------------+\n",
      "|2014-08-27 11:29:31|\n",
      "|2014-08-27 11:31:50|\n",
      "|2014-08-27 11:32:39|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .csv('/Users/snishad/SparkDemo/01-HelloSpark/data/sample.csv')\n",
    "\n",
    "# df.select('*').show()\n",
    "\n",
    "# df.select('Age', 'Gender', 'Country', 'state').show()\n",
    "\n",
    "# select all females\n",
    "df.where('Gender = \"Female\"').select('Timestamp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a9d04ae-345b-4d46-9e4f-977ff03532ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('37', 1)\n",
      "('2014-08-27 11:29:31', 1)('\"Female\"', 3)\n",
      "\n",
      "('\"IL\"', 2)\n",
      "('\"No\"', 79)\n",
      "('\"United States\"', 6)\n",
      "('\"Yes\"', 45)('NA', 21)\n",
      "\n",
      "('\"Often\"', 2)\n",
      "('\"6-25\"', 3)\n",
      "('\"M\"', 2)('\"Not sure\"', 2)\n",
      "\n",
      "('\"Rarely\"', 2)('\"Somewhat easy\"', 1)\n",
      "\n",
      "('\"Some of them\"', 4)\n",
      "('\"Don\\'t know\"', 18)\n",
      "('\"Maybe\"', 9)('2014-08-27 11:29:44', 1)\n",
      "\n",
      "('32', 1)\n",
      "('2014-08-27 11:29:37', 1)\n",
      "('44', 1)\n",
      "('\"Male\"', 4)('\"IN\"', 1)\n",
      "\n",
      "('\"More than 1000\"', 1)('\"Somewhat difficult\"', 3)\n",
      "\n",
      "('\"Canada\"', 2)('2014-08-27 11:29:46', 1)\n",
      "\n",
      "('\"United Kingdom\"', 1)('31', 2)\n",
      "\n",
      "('2014-08-27 11:30:22', 1)('\"26-100\"', 1)\n",
      "\n",
      "('\"TX\"', 1)('\"100-500\"', 2)\n",
      "\n",
      "('\"Never\"', 2)('2014-08-27 11:31:22', 1)\n",
      "('33', 1)\n",
      "\n",
      "('\"Sometimes\"', 3)('\"TN\"', 1)\n",
      "\n",
      "('2014-08-27 11:31:50', 1)\n",
      "('\"MI\"', 1)\n",
      "('35', 1)\n",
      "('2014-08-27 11:32:05', 1)\n",
      "('\"1-5\"', 2)\n",
      "('\"Very difficult\"', 1)\n",
      "('39', 1)\n",
      "('2014-08-27 11:32:39', 1)\n",
      "('42', 1)\n"
     ]
    }
   ],
   "source": [
    "# SPARK RDD\n",
    "\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# conf = SparkConf().setMaster('local[*]').setAppName('HelloRDD')\n",
    "\n",
    "# sc = SparkContext(conf=conf).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "rdd = sc.textFile('/Users/snishad/SparkDemo/02-HelloRDD/data/sample.csv')\n",
    "\n",
    "rdd1 = rdd.flatMap(lambda x: x.split(','))\n",
    "\n",
    "rdd2 = rdd1.map(lambda x: (x, 1))\n",
    "\n",
    "rdd3 = rdd2.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "rdd3.foreach(lambda x: print(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7567a092-7b2b-4b6a-8b0e-3291ebb626fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/02 10:51:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Country|count|\n",
      "+-------------+-----+\n",
      "|United States|    2|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SPARK SQL\n",
    "\n",
    "from pyspark.sql import *\n",
    "\n",
    "df = spark.read.option('inferSchema', 'true').option('header', 'true').csv('/Users/snishad/SparkDemo/03-HelloSparkSQL/data/sample.csv')\n",
    "\n",
    "df.createOrReplaceTempView('MyView')\n",
    "\n",
    "count_df = spark.sql('select Country, count(1) as count from MyView where Age > 40 group by Country')\n",
    "\n",
    "count_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3393328a-2638-475c-bab0-23951ccec9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|1/1/2000|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|1/1/2000|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|1/1/2000|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|1/1/2000|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|1/1/2000|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+--------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "struct<FL_DATE:string,OP_CARRIER:string,OP_CARRIER_FL_NUM:int,ORIGIN:string,ORIGIN_CITY_NAME:string,DEST:string,DEST_CITY_NAME:string,CRS_DEP_TIME:int,DEP_TIME:int,WHEELS_ON:int,TAXI_IN:int,CRS_ARR_TIME:int,ARR_TIME:int,CANCELLED:int,DISTANCE:int>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|ARR_TIME|CANCELLED|CRS_ARR_TIME|CRS_DEP_TIME|DEP_TIME|DEST|DEST_CITY_NAME|DISTANCE| FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|TAXI_IN|WHEELS_ON|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "|    1348|        0|        1400|        1115|    1113| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1451|   BOS|      Boston, MA|      5|     1343|\n",
      "|    1543|        0|        1559|        1315|    1311| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1479|   BOS|      Boston, MA|      7|     1536|\n",
      "|    1651|        0|        1721|        1415|    1414| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1857|   BOS|      Boston, MA|      9|     1642|\n",
      "|    2005|        0|        2013|        1715|    1720| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             1997|   BOS|      Boston, MA|     10|     1955|\n",
      "|    2240|        0|        2300|        2015|    2010| ATL|   Atlanta, GA|     946|1/1/2000|        DL|             2065|   BOS|      Boston, MA|     10|     2230|\n",
      "+--------+---------+------------+------------+--------+----+--------------+--------+--------+----------+-----------------+------+----------------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "struct<ARR_TIME:bigint,CANCELLED:bigint,CRS_ARR_TIME:bigint,CRS_DEP_TIME:bigint,DEP_TIME:bigint,DEST:string,DEST_CITY_NAME:string,DISTANCE:bigint,FL_DATE:string,OP_CARRIER:string,OP_CARRIER_FL_NUM:bigint,ORIGIN:string,ORIGIN_CITY_NAME:string,TAXI_IN:bigint,WHEELS_ON:bigint>\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "struct<FL_DATE:date,OP_CARRIER:string,OP_CARRIER_FL_NUM:int,ORIGIN:string,ORIGIN_CITY_NAME:string,DEST:string,DEST_CITY_NAME:string,CRS_DEP_TIME:int,DEP_TIME:int,WHEELS_ON:int,TAXI_IN:int,CRS_ARR_TIME:int,ARR_TIME:int,CANCELLED:int,DISTANCE:int>\n"
     ]
    }
   ],
   "source": [
    "# SPARK SCHEMA DEMO\n",
    "\n",
    "# csv\n",
    "csv_df = spark.read.format('csv').option('inferSchema', 'true').option('header', 'true').load('/Users/snishad/SparkDemo/04-SparkSchemaDemo/data/flight*.csv')\n",
    "csv_df.show(5)\n",
    "print(csv_df.schema.simpleString())\n",
    "\n",
    "# json\n",
    "json_df = spark.read.format('json').load('/Users/snishad/SparkDemo/04-SparkSchemaDemo/data/flight*.json')\n",
    "json_df.show(5)\n",
    "print(json_df.schema.simpleString())\n",
    "\n",
    "# parquet\n",
    "parquet_df = spark.read.format('parquet').load('/Users/snishad/SparkDemo/04-SparkSchemaDemo/data/flight*.parquet')\n",
    "parquet_df.show(5)\n",
    "print(parquet_df.schema.simpleString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b4d7c4e-a9b0-474d-8322-baa13f115848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|ORIGIN_CITY_NAME|DEST|DEST_CITY_NAME|CRS_DEP_TIME|DEP_TIME|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|CANCELLED|DISTANCE|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "|2000-01-01|        DL|             1451|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1115|    1113|     1343|      5|        1400|    1348|        0|     946|\n",
      "|2000-01-01|        DL|             1479|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1315|    1311|     1536|      7|        1559|    1543|        0|     946|\n",
      "|2000-01-01|        DL|             1857|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1415|    1414|     1642|      9|        1721|    1651|        0|     946|\n",
      "|2000-01-01|        DL|             1997|   BOS|      Boston, MA| ATL|   Atlanta, GA|        1715|    1720|     1955|     10|        2013|    2005|        0|     946|\n",
      "|2000-01-01|        DL|             2065|   BOS|      Boston, MA| ATL|   Atlanta, GA|        2015|    2010|     2230|     10|        2300|    2240|        0|     946|\n",
      "+----------+----------+-----------------+------+----------------+----+--------------+------------+--------+---------+-------+------------+--------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "struct<FL_DATE:date,OP_CARRIER:string,OP_CARRIER_FL_NUM:int,ORIGIN:string,ORIGIN_CITY_NAME:string,DEST:string,DEST_CITY_NAME:string,CRS_DEP_TIME:int,DEP_TIME:int,WHEELS_ON:int,TAXI_IN:int,CRS_ARR_TIME:int,ARR_TIME:int,CANCELLED:int,DISTANCE:int>\n"
     ]
    }
   ],
   "source": [
    "# TWO WAYS TO DEFINE SCHEMA \n",
    "# PROGRAMATICALLY AND DDL\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# PROGRAMATICALLY\n",
    "schema = StructType([\n",
    "        StructField(\"FL_DATE\", DateType()),\n",
    "        StructField(\"OP_CARRIER\", StringType()),\n",
    "        StructField(\"OP_CARRIER_FL_NUM\", IntegerType()),\n",
    "        StructField(\"ORIGIN\", StringType()),\n",
    "        StructField(\"ORIGIN_CITY_NAME\", StringType()),\n",
    "        StructField(\"DEST\", StringType()),\n",
    "        StructField(\"DEST_CITY_NAME\", StringType()),\n",
    "        StructField(\"CRS_DEP_TIME\", IntegerType()),\n",
    "        StructField(\"DEP_TIME\", IntegerType()),\n",
    "        StructField(\"WHEELS_ON\", IntegerType()),\n",
    "        StructField(\"TAXI_IN\", IntegerType()),\n",
    "        StructField(\"CRS_ARR_TIME\", IntegerType()),\n",
    "        StructField(\"ARR_TIME\", IntegerType()),\n",
    "        StructField(\"CANCELLED\", IntegerType()),\n",
    "        StructField(\"DISTANCE\", IntegerType())\n",
    "    ])\n",
    "\n",
    "\n",
    "# csv\n",
    "csv_df = spark.read.format('csv').option('inferSchema', 'true').option('header', 'true').schema(schema).option('dateFormat', 'm/d/y').option('mode', 'FAILFAST').load('/Users/snishad/SparkDemo/04-SparkSchemaDemo/data/flight*.csv')\n",
    "csv_df.show(5)\n",
    "print(csv_df.schema.simpleString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe55cfa-4f95-46e8-a868-866ef48b1e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|ORIGIN|count|\n",
      "+------+-----+\n",
      "|   BGM|   93|\n",
      "|   DLG|   18|\n",
      "|   MSY| 4150|\n",
      "|   GEG| 1028|\n",
      "|   SNA| 3730|\n",
      "|   BUR| 2252|\n",
      "|   GTF|  215|\n",
      "|   GRB|  224|\n",
      "|   GRR|  771|\n",
      "|   PSG|   62|\n",
      "|   EUG|  185|\n",
      "|   PVD| 2020|\n",
      "|   GSO| 1265|\n",
      "|   MYR|  164|\n",
      "|   OAK| 4597|\n",
      "|   MSN|  385|\n",
      "|   FAR|  231|\n",
      "|   SCC|   44|\n",
      "|   DCA| 6818|\n",
      "|   MLU|  181|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions = 2\n",
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|235238|\n",
      "|                   1|235239|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SPARK DATA SINK\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "df = spark.read.format('parquet').load('/Users/snishad/SparkDemo/04-SparkSchemaDemo/data/flight*.parquet')\n",
    "\n",
    "df.groupBy('ORIGIN').count().show()\n",
    "\n",
    "df = df.repartition(2)\n",
    "\n",
    "# df.write.format('json').mode('overwrite').save('/Users/snishad/SparkDemo/05-DataSinkDemo/upload/')\n",
    "\n",
    "# Check the partitions\n",
    "\n",
    "print(f'Number of partitions = {df.rdd.getNumPartitions()}')\n",
    "\n",
    "# Count data in each partitions\n",
    "\n",
    "df.groupBy(spark_partition_id()).count().show()\n",
    "\n",
    "# write with logical partition by and max records per file\n",
    "# df.write.format('json').mode('overwrite').partitionBy('OP_CARRIER', 'ORIGIN').option('maxRecordsPerFile', 1000).save('/Users/snishad/SparkDemo/05-DataSinkDemo/upload/')\n",
    "# df.write.format('json').mode('overwrite').partitionBy('OP_CARRIER', 'ORIGIN').save('/Users/snishad/SparkDemo/05-DataSinkDemo/upload/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f6dcd49-9a9e-4a0c-a9ef-168264690e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+--------------+\n",
      "|ORIGIN|OP_CARRIER|ORIGIN_CITY_NAME|DEST_CITY_NAME|\n",
      "+------+----------+----------------+--------------+\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "|   OAK|        AS|     Oakland, CA|   Seattle, WA|\n",
      "+------+----------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# SPARK TRANSFORMATIONS\n",
    "\n",
    "from pyspark.sql.functions import column, col\n",
    "\n",
    "# Four ways to reffer a column in DF\n",
    "\n",
    "df.select(column('ORIGIN'), col('OP_CARRIER'), df.ORIGIN_CITY_NAME, 'DEST_CITY_NAME').show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d2126e2-d92e-4d28-97cd-420bb780bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+\n",
      "| name|day|month|  year|\n",
      "+-----+---+-----+------+\n",
      "| Ravi| 28|    1|  2002|\n",
      "|Abdul| 23|    5|2071.0|\n",
      "| John| 12|   12|2006.0|\n",
      "| Rosy|  7|    8|2053.0|\n",
      "|Abdul| 23|    5|2071.0|\n",
      "+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data_list = [(\"Ravi\", \"28\", \"1\", \"2002\"),\n",
    "                 (\"Abdul\", \"23\", \"5\", \"81\"),  # 1981\n",
    "                 (\"John\", \"12\", \"12\", \"6\"),  # 2006\n",
    "                 (\"Rosy\", \"7\", \"8\", \"63\"),  # 1963\n",
    "                 (\"Abdul\", \"23\", \"5\", \"81\")]  # 1981\n",
    "\n",
    "\n",
    "name_df = spark.createDataFrame(data_list).toDF('name', 'day', 'month', 'year')\n",
    "\n",
    "\n",
    "# Case when then\n",
    "\n",
    "name_df = name_df.withColumn('year', expr(\"\"\"\n",
    "case when year < 21 then year + 2000\n",
    "when year < 100 then year + 1990\n",
    "else year\n",
    "end\"\"\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
